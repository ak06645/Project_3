{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Recurrent Neural Networks\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Welcome to Project 3: Recurrent Neural Networks!<br>\n",
    "    <br>\n",
    "    In this project you will work with a basic RNN architecture for text generation, namely Sequence to Seqeuence Models. The name points out that these models take a sequence as input and return another sequence as output (see Fig. 1).<br>\n",
    "    <br>\n",
    "    Sequence to Vector Models are similar, but return only a single output in the final time step (see Fig. 2). The latter architecture is, for example, suitable for sequence classification.\n",
    "</div>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <th><img src=\"rnn_seq_2_seq.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "        <th><img src=\"rnn_seq_2_vec.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fig. 1: Sequence to Sequence RNN with one hidden layer.</th>\n",
    "        <th>Fig. 2: Sequence to Vector RNN with one hidden layer.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Read the text files <font face='courier'>hamlet_1.txt</font>, <font face='courier'>hamlet_2.txt</font> and <font face='courier'>hamlet_3.txt</font>.<br>\n",
    "    <br>\n",
    "    Store their content in variables <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>, respectively.<br>\n",
    "    <br>\n",
    "    Be aware that all files are UTF-8 encoded. Maybe you find <a href='https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files'>this link</a> helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_text = open('hamlet_1.txt', encoding=\"utf-8\").read()\n",
    "hamlet_2_text = open('hamlet_2.txt', encoding=\"utf-8\").read()\n",
    "hamlet_3_text = open('hamlet_3.txt', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Print the first 325 characters of <font face='courier'>hamlet_1_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamlet_1_text = open('hamlet_1.txt', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hamlet_1_text.read(325))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      " Barnardo. Who's there?\n",
      "Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      " Bar. Long liue the King\n",
      "\n",
      " Fran. Barnardo?\n",
      "Bar. He\n",
      "\n",
      " Fran. You come most carefully vpon your houre\n",
      "\n",
      " Bar. 'Tis now strook twelue, get thee to bed Francisco"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "characters = 0 \n",
    "for line in hamlet_1_text:\n",
    "    print(line, end='')\n",
    "    characters += sum(len(word) for word in line)\n",
    "    if characters == 325:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should see the following output:\n",
    "\n",
    "```\n",
    "The Tragedie of Hamlet\n",
    "\n",
    "Actus Primus. Scoena Prima.\n",
    "\n",
    "Enter Barnardo and Francisco two Centinels.\n",
    "\n",
    " Barnardo. Who's there?\n",
    "Fran. Nay answer me: Stand & vnfold\n",
    "your selfe\n",
    "\n",
    " Bar. Long liue the King\n",
    "\n",
    " Fran. Barnardo?\n",
    "Bar. He\n",
    "\n",
    " Fran. You come most carefully vpon your houre\n",
    "\n",
    " Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Instantiate an object <font face='courier'>tokenizer</font> of type <font face='courier'>tf.keras.preprocessing.text.Tokenzier</font>.<br>\n",
    "    <br>\n",
    "    Submit the argument <font face='courier'>char_level=True</font> to the constructor.<br>\n",
    "    <br>\n",
    "    Helpful information can be found <a href='https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer'>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The object <font face='courier'>tokenizer</font> will ultimately be used to encode the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font> as integer sequences in which each single number represents a specific character.<br>\n",
    "    <br>\n",
    "    Call the method <font face='courier'>fit_on_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    In doing so, pass a list that contains exactly the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer.fit_on_texts([hamlet_1_text, hamlet_2_text, hamlet_3_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The above call creates a vocabulary in <font face='courier'>tokenizer</font>. This vocabulary assigns a positive integer to each unique character in the texts that were passed to the <font face='courier'>fit_on_texts</font> method.<br>\n",
    "    <br>\n",
    "    Display the attribute <font face='courier'>word_index</font> of <font face='courier'>tokenizer</font> which contains the created vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'o': 4,\n",
       " 'a': 5,\n",
       " 'h': 6,\n",
       " 'i': 7,\n",
       " 'n': 8,\n",
       " 's': 9,\n",
       " 'r': 10,\n",
       " 'l': 11,\n",
       " '\\n': 12,\n",
       " 'u': 13,\n",
       " 'd': 14,\n",
       " 'm': 15,\n",
       " 'y': 16,\n",
       " ',': 17,\n",
       " 'w': 18,\n",
       " 'f': 19,\n",
       " 'c': 20,\n",
       " 'g': 21,\n",
       " '.': 22,\n",
       " 'p': 23,\n",
       " 'b': 24,\n",
       " 'k': 25,\n",
       " \"'\": 26,\n",
       " ':': 27,\n",
       " 'v': 28,\n",
       " '?': 29,\n",
       " ';': 30,\n",
       " 'q': 31,\n",
       " 'x': 32,\n",
       " '-': 33,\n",
       " 'z': 34,\n",
       " '(': 35,\n",
       " ')': 36,\n",
       " '&': 37,\n",
       " '!': 38,\n",
       " '[': 39,\n",
       " ']': 40,\n",
       " '1': 41,\n",
       " 'j': 42}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have got the following output:\n",
    "\n",
    "```\n",
    "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'n': 8, 's': 9, 'r': 10, 'l': 11, '\\n': 12, 'u': 13, 'd': 14, 'm': 15, 'y': 16, ',': 17, 'w': 18, 'f': 19, 'c': 20, 'g': 21, '.': 22, 'p': 23, 'b': 24, 'k': 25, \"'\": 26, ':': 27, 'v': 28, '?': 29, ';': 30, 'q': 31, 'x': 32, '-': 33, 'z': 34, '(': 35, ')': 36, '&': 37, '!': 38, '[': 39, ']': 40, '1': 41, 'j': 42}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compute the length of the vocabulary, store it in the variable <font face='courier'>max_id</font> and display this variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "max_id = len(tokenizer.word_index)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Now, use the method <font face='courier'>texts_to_sequences</font> of <font face='courier'>tokenzier</font> to encode <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.<br>\n",
    "    <br>\n",
    "    Store the respective coded strings in <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.<br>\n",
    "    <br>\n",
    "    Convert all three lists to the format <font face='courier'>numpy.array</font> and subtract <font face='courier'>1</font> from all entries so that all values range between <font face='courier'>0</font> and <font face='courier'>max_id - 1</font> afterwards.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded = np.squeeze(np.array(tokenizer.texts_to_sequences(hamlet_1_text)) - 1)\n",
    "hamlet_2_encoded = np.squeeze(np.array(tokenizer.texts_to_sequences(hamlet_2_text)) - 1)\n",
    "hamlet_3_encoded = np.squeeze(np.array(tokenizer.texts_to_sequences(hamlet_3_text)) - 1)\n",
    "\n",
    "\n",
    "#YOUR CODE\n",
    "# hamlet_1_encoded = np.array(tokenizer.texts_to_sequences(hamlet_1_text)) - 1\n",
    "# hamlet_2_encoded = np.array(tokenizer.texts_to_sequences(hamlet_2_text)) - 1\n",
    "# hamlet_3_encoded = np.array(tokenizer.texts_to_sequences(hamlet_3_text)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first <font face='courier'>325</font> entries of <font face='courier'>hamlet_1_encoded</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamlet_1_encoded = hamlet_1_encoded.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64148,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_1_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5,  1,  0,  2,  9,  4, 20,  1, 13,  6,  1,  0,  3, 18,  0,  5,\n",
       "        4, 14, 10,  1,  2, 11, 11,  4, 19,  2, 12,  8,  0, 22,  9,  6, 14,\n",
       "       12,  8, 21,  0,  8, 19,  3,  1,  7,  4,  0, 22,  9,  6, 14,  4, 21,\n",
       "       11, 11,  1,  7,  2,  1,  9,  0, 23,  4,  9,  7,  4,  9, 13,  3,  0,\n",
       "        4,  7, 13,  0, 18,  9,  4,  7, 19,  6,  8, 19,  3,  0,  2, 17,  3,\n",
       "        0, 19,  1,  7,  2,  6,  7,  1, 10,  8, 21, 11, 11,  0, 23,  4,  9,\n",
       "        7,  4,  9, 13,  3, 21,  0, 17,  5,  3, 25,  8,  0,  2,  5,  1,  9,\n",
       "        1, 28, 11, 18,  9,  4,  7, 21,  0,  7,  4, 15,  0,  4,  7,  8, 17,\n",
       "        1,  9,  0, 14,  1, 26,  0,  8,  2,  4,  7, 13,  0, 36,  0, 27,  7,\n",
       "       18,  3, 10, 13, 11, 15,  3, 12,  9,  0,  8,  1, 10, 18,  1, 11, 11,\n",
       "        0, 23,  4,  9, 21,  0, 10,  3,  7, 20,  0, 10,  6, 12,  1,  0,  2,\n",
       "        5,  1,  0, 24,  6,  7, 20, 11, 11,  0, 18,  9,  4,  7, 21,  0, 23,\n",
       "        4,  9,  7,  4,  9, 13,  3, 28, 11, 23,  4,  9, 21,  0,  5,  1, 11,\n",
       "       11,  0, 18,  9,  4,  7, 21,  0, 15,  3, 12,  0, 19,  3, 14,  1,  0,\n",
       "       14,  3,  8,  2,  0, 19,  4,  9,  1, 18, 12, 10, 10, 15,  0, 27, 22,\n",
       "        3,  7,  0, 15,  3, 12,  9,  0,  5,  3, 12,  9,  1, 11, 11,  0, 23,\n",
       "        4,  9, 21,  0, 25,  2,  6,  8,  0,  7,  3, 17,  0,  8,  2,  9,  3,\n",
       "        3, 24,  0,  2, 17,  1, 10, 12,  1, 16,  0, 20,  1,  2,  0,  2,  5,\n",
       "        1,  1,  0,  2,  3,  0, 23,  1, 13,  0, 18,  9,  4,  7, 19,  6,  8,\n",
       "       19,  3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded[:325]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should be as follows:\n",
    "\n",
    "```\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4  9  7  4  9 13  3 21  0 17  5  3 25  8  0  2  5  1  9  1\n",
    " 28 11 18  9  4  7 21  0  7  4 15  0  4  7  8 17  1  9  0 14  1 26  0  8\n",
    "  2  4  7 13  0 36  0 27  7 18  3 10 13 11 15  3 12  9  0  8  1 10 18  1\n",
    " 11 11  0 23  4  9 21  0 10  3  7 20  0 10  6 12  1  0  2  5  1  0 24  6\n",
    "  7 20 11 11  0 18  9  4  7 21  0 23  4  9  7  4  9 13  3 28 11 23  4  9\n",
    " 21  0  5  1 11 11  0 18  9  4  7 21  0 15  3 12  0 19  3 14  1  0 14  3\n",
    "  8  2  0 19  4  9  1 18 12 10 10 15  0 27 22  3  7  0 15  3 12  9  0  5\n",
    "  3 12  9  1 11 11  0 23  4  9 21  0 25  2  6  8  0  7  3 17  0  8  2  9\n",
    "  3  3 24  0  2 17  1 10 12  1 16  0 20  1  2  0  2  5  1  1  0  2  3  0\n",
    " 23  1 13  0 18  9  4  7 19  6  8 19  3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The original texts can be recovered via the method <font face='courier'>sequences_to_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    Texts recovered in this way are all lower case and each original character is followed by a blank space.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>sequences_to_texts</font> to <font face='courier'>hamlet_1_encoded + 1</font> and store the result in <font face='courier'>hamlet_1_decoded</font>.<br>\n",
    "    <br>\n",
    "    Afterfwards, display the first <font face='courier'>649</font> characters of <font face='courier'>hamlet_1_decoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e   t r a g e d i e   o f   h a m l e t \n",
      " \n",
      " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
      " \n",
      " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
      " \n",
      "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
      " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
      " y o u r   s e l f e \n",
      " \n",
      "   b a r .   l o n g   l i u e   t h e   k i n g \n",
      " \n",
      "   f r a n .   b a r n a r d o ? \n",
      " b a r .   h e \n",
      " \n",
      "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
      " \n",
      "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_decoded = tokenizer.sequences_to_texts([hamlet_1_encoded + 1])\n",
    "# hamlet_1_decoded = tokenizer.sequences_to_texts([x+1 for x in hamlet_1_encoded])\n",
    "\n",
    "print(' '.join(hamlet_1_decoded)[:649])\n",
    "# count = 0 \n",
    "# for ch in hamlet_1_decoded:\n",
    "#     print(ch, end='')\n",
    "#     count = count+1\n",
    "#     if count == 649:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look like this:\n",
    "\n",
    "```\n",
    "t h e   t r a g e d i e   o f   h a m l e t \n",
    " \n",
    " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
    " \n",
    " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
    " \n",
    "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
    " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
    " y o u r   s e l f e \n",
    " \n",
    "   b a r .   l o n g   l i u e   t h e   k i n g \n",
    " \n",
    "   f r a n .   b a r n a r d o ? \n",
    " b a r .   h e \n",
    " \n",
    "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
    " \n",
    "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Create three objects <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> of type <font face='courier'>tf.data.dataset</font> by applying the method<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.from_tensor_slices</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices'>this link</a>) to <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = tf.data.Dataset.from_tensor_slices(hamlet_1_encoded)\n",
    "hamlet_2_dataset = tf.data.Dataset.from_tensor_slices(hamlet_2_encoded)\n",
    "hamlet_3_dataset = tf.data.Dataset.from_tensor_slices(hamlet_3_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first ten elements of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "count = 0\n",
    "for ele in hamlet_1_dataset:\n",
    "    print(ele)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have produced the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, each item of <font face='courier'>hamlet_1_dataset</font> is an integer tensor including one single value.<br>\n",
    "    <br>\n",
    "    Accordingly, <font face='courier'>hamlet_1_dataset</font> has <font face='courier'>len(hamlet_1_encoded)</font> elements in total (the same applies in case of the other two datasets).<br>\n",
    "    <br>\n",
    "    In the following, you will train a recurrent neural network which gets a coded string of length <font face='courier'>T = 100</font> and predicts subsequent characters in all time steps.<br>\n",
    "    <br>\n",
    "    Accordingly, we will first transform our three datasets such that their elements become one-dimensional tensors of length <font face='courier'>window_length = T + 1</font>.<br>\n",
    "    <br>\n",
    "    Initialize <font face='courier'>T</font> and <font face='courier'>window_length</font> as described.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "T = 100\n",
    "window_length = T + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use the method <font face='courier'>tf.data.Dataset.window</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window'>this link</a>) to get items that feature the desired length.<br>\n",
    "    <br>\n",
    "    Call the method using the arguments <font face='courier'>shift = 1</font> and <font face='courier'>drop_remainder = True</font>.<br>\n",
    "    <br>\n",
    "    The first-mentioned argument <font face='courier'>shift = 1</font> ensures that the first item of the transformed dataset contains the elements <font face='courier'>0,...,window_length - 1</font> of the original dataset, the next item <font face='courier'>1,...,window_length</font>, and so on.<br>\n",
    "    <br>\n",
    "    In other words: A window of length <font face='courier'>window_length</font> slides with feed <font face='courier'>shift</font> over the original dataset and extracts sequence by sequence until the end of the dataset is reached.<br>\n",
    "    <br>\n",
    "    The latter argument <font face='courier'>drop_remainder = True</font> ensures that no shorter sequences are extracted towards the end of the dataset, where the window could potentially slide beyond the end of the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = hamlet_1_dataset.window(size = window_length, shift = 1, drop_remainder = True)\n",
    "hamlet_2_dataset = hamlet_2_dataset.window(size = window_length, shift = 1, drop_remainder = True)\n",
    "hamlet_3_dataset = hamlet_3_dataset.window(size = window_length, shift = 1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for window in hamlet_1_dataset.take(1):\n",
    "    print(window)\n",
    "    for item in window.take(10):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have obtained an output like this:\n",
    "\n",
    "```\n",
    "<_VariantDataset shapes: (), types: tf.int32>\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, the transformed datasets <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> have now elements which are again objects of type <font face='courier'>tf.data.Dataset</font> (or of a derived class).<br>\n",
    "-    <br>\n",
    "    Each of these sub-datasets <font face='courier'>window</font> contains a number of <font face='courier'>window_size</font> single-valued tensors.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.flat_map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map'>this link</a>) to all three datasets to transform the sub-datasets <font face='courier'>window</font> into one-dimensional tensors of length <font face='courier'>window_length</font>.<br>\n",
    "    <br>\n",
    "    Pass a function which maps <font face='courier'>window</font> to <font face='courier'>window.batch(window_length)</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = hamlet_1_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "hamlet_2_dataset = hamlet_2_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "hamlet_3_dataset = hamlet_3_dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first element of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
      "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
      " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
      " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
      " 11 11  0 23  4], shape=(101,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "for window in hamlet_1_dataset.take(1):\n",
    "    print(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should get the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4], shape=(101,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.concatenate</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#concatenate'>this link</a>) to merge<br>\n",
    "    <br>\n",
    "    <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> to a single dataset <font face='courier'>hamlet_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset = hamlet_1_dataset.concatenate(hamlet_2_dataset)\n",
    "hamlet_dataset = hamlet_dataset.concatenate(hamlet_3_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Set <font face='courier'>batch_size = 32</font>.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>tf.data.Dataset.repeat</font> (without argument),<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.shuffle</font> (with <font face='courier'>buffer_size = 1000</font>)<br>\n",
    "    <br>\n",
    "    and finally <font face='courier'>tf.data.Dataset.batch</font> (with <font face='courier'>drop_remainder=True</font>).\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# YOUR CODE\n",
    "batch_size = 32\n",
    "hamlet_dataset = hamlet_dataset.repeat()\n",
    "hamlet_dataset = hamlet_dataset.shuffle(buffer_size = 10000)\n",
    "hamlet_dataset = hamlet_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Our dataset contains now two-dimensional tensors <font face='courier'>window_batch</font> of size <font face='courier'>(32, 101)</font>.<br>\n",
    "    <br>\n",
    "    Each slice <font face='courier'>window_batch[i, :]</font> corresponds to a training example.<br>\n",
    "    <br>\n",
    "    Here, we still need to subdivide the training examples into inputs and outputs.<br>\n",
    "    <br>\n",
    "    Each single encoded character <font face='courier'>window_batch[i, j]</font> (for <font face='courier'>j=0,...,99</font>) is an  input $\\mathbf{x}^{<t>(i)}$ in a time step<br>\n",
    "    <br>\n",
    "    and the associated output is <font face='courier'>window_batch[i, j + 1]</font> which corresponds to $\\mathbf{y}^{<t>(i)}$.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map'>this link</a>) to <font face='courier'>hamlet_dataset</font><br>.\n",
    "    <br>\n",
    "    Each batch <font face='courier'>window_batch</font> shall be mapped to a tuple of two tensors of size <font face='courier'>(32, 100)</font>.<br>\n",
    "    <br>\n",
    "    The <font face='courier'>[i, :]</font>-th slice of the first tensor shall contain the entries <font face='courier'>window_batch[i, 0:100]</font>.<br>\n",
    "    <br>\n",
    "    The corresponding slice of the second tensor shall contain <font face='courier'>window_batch[i, 1:101]</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "# def window_batch_mapping(window_batch):\n",
    "#     global k\n",
    "# #     temp = (window_batch[k, 0:100], window_batch[k, 1:101])\n",
    "#     for i in window_batch:\n",
    "#         print(i)\n",
    "#     print(k)\n",
    "# #     window_batch_new = []\n",
    "# #     for i in window_batch:\n",
    "# #         temp = (window_batch[i, 0:100], window_batch[i, 1:101])\n",
    "# #         window_batch_new.append(temp)\n",
    "#     return window_batch\n",
    "    \n",
    "# # hamlet_dataset = hamlet_dataset.map(lambda window_batch: (window_batch[0, 0:100], window_batch[1, 1:101]))\n",
    "# #hamlet_dataset.map(lambda window_batch: (window_batch[0, 0:100], window_batch[1, 1:101]))\n",
    "# hamlet_dataset = hamlet_dataset.map(window_batch_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_dataset = hamlet_dataset.map(lambda window_batch: (window_batch[:, 0:100], window_batch[:, 1:101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "#hamlet_dataset = hamlet_dataset.map(lambda window_batch: (window_batch[0:100, :], window_batch[1:101, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(32, None), dtype=tf.int32, name=None), TensorSpec(shape=(32, None), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100)\n"
     ]
    }
   ],
   "source": [
    "for window_batch in hamlet_dataset.take(1):\n",
    "    print((window_batch[1].numpy()+1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for window_batch in hamlet_dataset.take(1):\n",
    "    print(type(window_batch[0][0, :].numpy().ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  p o l l a x   o n   t h e   i c e . \n",
      " ' t i s   s t r a n g e \n",
      " \n",
      "   m a r .   t h u s   t w i c e   b e f o r e ,   a n d   i u s t   a t   t h i s   d e a d   h o u r e , \n",
      " w i t h   m a r t i a l\n",
      "\n",
      "p o l l a x   o n   t h e   i c e . \n",
      " ' t i s   s t r a n g e \n",
      " \n",
      "   m a r .   t h u s   t w i c e   b e f o r e ,   a n d   i u s t   a t   t h i s   d e a d   h o u r e , \n",
      " w i t h   m a r t i a l l\n"
     ]
    }
   ],
   "source": [
    "for window_batch in hamlet_dataset.take(1):\n",
    "    [x] = tokenizer.sequences_to_texts([window_batch[0][0, :].numpy() + 1])\n",
    "    [y] = tokenizer.sequences_to_texts([window_batch[1][0, :].numpy() + 1])\n",
    "#     x = tokenizer.sequences_to_texts(window_batch[0].numpy() + 1)\n",
    "#     y = tokenizer.sequences_to_texts(window_batch[1].numpy() + 1)\n",
    "    print(x)\n",
    "    print()\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_x = ''.join(x1 for x1 in x if x1!='\\n')\n",
    "output_y = ''.join(x1 for x1 in y if x1!='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  p o l l a x   o n   t h e   i c e .  ' t i s   s t r a n g e     m a r .   t h u s   t w i c e   b e f o r e ,   a n d   i u s t   a t   t h i s   d e a d   h o u r e ,  w i t h   m a r t i a l\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"p o l l a x   o n   t h e   i c e .  ' t i s   s t r a n g e     m a r .   t h u s   t w i c e   b e f o r e ,   a n d   i u s t   a t   t h i s   d e a d   h o u r e ,  w i t h   m a r t i a l l\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look as follows:\n",
    "\n",
    "```\n",
    "  o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p\n",
    "\n",
    "o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply <font face='courier'>tf.data.Dataset.map</font> again to <font face='courier'>hamlet_dataset</font> to map each element <font face='courier'>(X_batch, Y_batch)</font> to a new tuple.<br>\n",
    "    <br>\n",
    "    In the new tuple, <font face='courier'>Y_batch</font> shall remain unchanged while <font face='courier'>X_batch</font> undergoes another encoding via <font face='courier'>tf.one_hot</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/one_hot'>this link</a>).<br>\n",
    "    <br>\n",
    "    Find out, which value <font face='courier'>depth</font> you need to pass <font face='courier'>tf.one_hot</font> in addition to <font face='courier'>X_batch</font>.<br>\n",
    "    <br>\n",
    "    Hint: You already computed the correct value above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset = hamlet_dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 42)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in hamlet_dataset.take(1):\n",
    "    print(X_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(32, None, 42), dtype=tf.float32, name=None), TensorSpec(shape=(32, None), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.prefetch</font> (with <font face='courier'>buffer_size = 1</font>) to <font face='courier'>hamlet_dataset</font> to get your dataset ready for training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset = hamlet_dataset.prefetch(buffer_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(32, None, 42), dtype=tf.float32, name=None), TensorSpec(shape=(32, None), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Deduce a formula for the number of elements in <font face='courier'>hamlet_dataset</font>.<br>\n",
    "    <br>\n",
    "    Display the result and store it in <font face='courier'>steps_per_epoch</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5014\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "steps_per_epoch= int(((len(hamlet_1_encoded)+len(hamlet_2_encoded)+len(hamlet_3_encoded))-3*T)/batch_size)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The number of items in the dataset should be as follows:\n",
    "\n",
    "```\n",
    "5014\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use <font face='courier'>keras.models.Sequential</font> to define a <font face='courier'>model</font> featuring<br>\n",
    "    <br>\n",
    "    two hidden GRU layers (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU'>this link</a>) with <font face='courier'>128</font> neurons each, as well as<br>\n",
    "    <br>\n",
    "    a fully connected output layer (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'>this link</a>) with <font face='courier'>max_id</font> neurons and <font face='courier'>softmax</font> activation function.<br>\n",
    "    <br>\n",
    "    This model corresponds to the RNN displayed in Fig. 1 with an additional hidden GRU layer.<br>\n",
    "    <br>\n",
    "    To make the model generate outputs in each time step, the output layer must be enclosed by a <font face='courier'>keras.layers.TimeDistributed</font> wrapper (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed'>this link</a>).<br>\n",
    "    <br>\n",
    "    Without the layer, the model would generate only a single output in the very last time step (as the one displaye in Fig. 2).<br>\n",
    "    <br>\n",
    "    In case of the GRU layers, you should add <font face='courier'>return_sequences = True</font> for the same purpose.<br>\n",
    "    <br>\n",
    "    Also recall that the input layer needs an argument <font face='courier'>input_shape=[None, max_id]</font> (where <font face='courier'>None</font> represents the temporal dimension of the input sequence).<br>\n",
    "    <br>\n",
    "    During training, we could just as well replace <font face='courier'>None</font> with <font face='courier'>T</font> as all sequences in the training data have identical length.<br>\n",
    "    <br>\n",
    "    However, passing <font face='courier'>None</font> ensures that the model will accept arbitrarily long sequences later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.GRU(128, return_sequences = True, input_shape=[None, max_id]))\n",
    "model.add(keras.layers.GRU(128, return_sequences = True))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compile <font face='courier'>model</font> using <font face='courier'>sparse_categorical_crossentropy</font> and <font face='courier'>adam</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Train <font face='courier'>model</font> for one epoch. Don't forget to pass the <font face='courier'>steps_per_epoch</font> argument.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "history = model.fit(hamlet_dataset,epochs=1,steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    You might have observed that even a single epoch takes quite some time.<br>\n",
    "    <br>\n",
    "    Create a file <font face='courier'>hamlet_rnn.py</font> and train the model for <font face='courier'>20</font> epochs on the GPU cluster.<br>\n",
    "    <br>\n",
    "    In doing so, use a callback<br>\n",
    "    <br>\n",
    "    <font face='courier'>keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)</font><br>\n",
    "    <br>\n",
    "    (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping'>this link</a>). Save your model as  <font face='courier'>hamlet_model.h5</font> and load it in the following step for further use in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('one_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('hamlet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_new = model.fit(hamlet_dataset, epochs=32, steps_per_epoch=steps_per_epoch, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hamlet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_4 (GRU)                 (None, None, 128)         66048     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, None, 128)         99072     \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 42)         5418      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 170,538\n",
      "Trainable params: 170,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Write a function <font face='courier'>preprocess</font> that takes a argument a list <font face='courier'>texts</font> containing a single string.<br>\n",
    "    <br>\n",
    "    Inside this function, use <font face='courier'>tokenizer</font> again to encode the string in <font face='courier'>texts</font> as a NumPy-Array <font face='courier'>X</font>.<br>\n",
    "    <br>\n",
    "    The return value of <font face='courier'>preprocess</font> shall be the one-hot encoded version of <font face='courier'>X</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "def preprocess(texts):\n",
    "#     x = np.squeeze(np.array(tokenizer.texts_to_sequences(texts)) - 1)\n",
    "#     dataset_text = tf.data.Dataset.from_tensor_slices(x)\n",
    "    x = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    one_hot_x= tf.one_hot(x, max_id)\n",
    "#     one_hot_x = one_hot_x\n",
    "    return one_hot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1, 42), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('Hamlet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>next_char</font> takes as argument a string <font face='courier'>text</font> and a positive number <font face='courier'>temperature</font><br>\n",
    "    <br>\n",
    "    with the goal to generate the next character after <font face='courier'>text</font>.<br>\n",
    "    <br>\n",
    "    Replace all placeholders <font face='courier'>None</font> as follows:<br>\n",
    "    <br>\n",
    "    First, apply <font face='courier'>preprocess</font> to encode <font face='courier'>text</font> and store the result in <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Second, apply <font face='courier'>model.predict</font> the generate outputs given the input sequence <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Store the output from the last time step in <font face='courier'>y_proba</font>.<br>\n",
    "    <br>\n",
    "    <font face='courier'>rescaled_logits</font> and <font face='courier'>char_id</font> are finally used to generate a one-element sample from  $\\{1,\\dots,\\mathrm{max\\_id}\\}$.<br>\n",
    "    <br>\n",
    "    The generated sample is the encoded version of the character to be generated.<br>\n",
    "    <br>\n",
    "    Hence, <font face='courier'>char_id.numpy()</font> will return the character itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = new_model.predict(X_new)[0,-1:,:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>complete_text</font> takes a string <font face='courier'>text</font> and shall append <font face='courier'>n_char</font> subsequent characters generated by your RNN.<br>\n",
    "    <br>\n",
    "    Complete the function accordingly. The argument <font face='courier'>temperature</font> just needs to be passed to <font face='courier'>next_char</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Starting from the initial string <font face='courier'>'Hamlet'</font>, use <font face='courier'>complete_text</font> to generate a text with <font face='courier'>1000</font> characters.<br>\n",
    "    <br>\n",
    "    You can vary the argument <font face='courier'>temperature</font> to modify the distribution from which new characters are drawn.<br>\n",
    "    <br>\n",
    "    Values close to zero encourage characters that have a high probability according to the distribution generated by your RNN.<br>\n",
    "    <br>\n",
    "    If <font face='courier'>temperature</font> is too high, new characters are drawn according to a uniform distribution on the entire vocabulary, which is not desirable.<br>\n",
    "    <br>\n",
    "    You can, for example, try different values between <font face='courier'>0</font> and <font face='courier'>2</font> and evaluate generated texts based on how plausible they appear to you.<br>\n",
    "    <br>\n",
    "    Display your generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 559ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 997us/step\n",
      "2/2 [==============================] - 0s 995us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 992us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 1ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "21/21 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "22/22 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "26/26 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hamleteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeieeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeaeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaeeeeeeeeeeeeeeeeeeeeeeeeeee'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(complete_text(\"Hamlet\",1000,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "output = []\n",
    "for i in np.arange(0.1,2.1,0.1):\n",
    "    output.append(complete_text(\"Hamlet\",1000,i))\n",
    "\n",
    "with open(\"output.txt\", \"w+\") as f:\n",
    "    for idx,i in enumerate(output):\n",
    "        f.write(f\"For {idx}\\n\\n{i}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Submit your completed notebook not later than on January 15th, 2023.<br>\n",
    "    <br>\n",
    "    Optional: Train an RNN on a text corpus of your own choice and use your own RNN to generate text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
