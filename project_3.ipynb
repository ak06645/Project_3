{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Recurrent Neural Networks\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Welcome to Project 3: Recurrent Neural Networks!<br>\n",
    "    <br>\n",
    "    In this project you will work with a basic RNN architecture for text generation, namely Sequence to Seqeuence Models. The name points out that these models take a sequence as input and return another sequence as output (see Fig. 1).<br>\n",
    "    <br>\n",
    "    Sequence to Vector Models are similar, but return only a single output in the final time step (see Fig. 2). The latter architecture is, for example, suitable for sequence classification.\n",
    "</div>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <th><img src=\"rnn_seq_2_seq.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "        <th><img src=\"rnn_seq_2_vec.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fig. 1: Sequence to Sequence RNN with one hidden layer.</th>\n",
    "        <th>Fig. 2: Sequence to Vector RNN with one hidden layer.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Read the text files <font face='courier'>hamlet_1.txt</font>, <font face='courier'>hamlet_2.txt</font> and <font face='courier'>hamlet_3.txt</font>.<br>\n",
    "    <br>\n",
    "    Store their content in variables <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>, respectively.<br>\n",
    "    <br>\n",
    "    Be aware that all files are UTF-8 encoded. Maybe you find <a href='https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files'>this link</a> helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_text = open('hamlet_1.txt', encoding=\"utf-8\").read()\n",
    "hamlet_2_text = open('hamlet_2.txt', encoding=\"utf-8\").read()\n",
    "hamlet_3_text = open('hamlet_3.txt', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Print the first 325 characters of <font face='courier'>hamlet_1_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamlet_1_text = open('hamlet_1.txt', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hamlet_1_text.read(325))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      " Barnardo. Who's there?\n",
      "Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      " Bar. Long liue the King\n",
      "\n",
      " Fran. Barnardo?\n",
      "Bar. He\n",
      "\n",
      " Fran. You come most carefully vpon your houre\n",
      "\n",
      " Bar. 'Tis now strook twelue, get thee to bed Francisco"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "characters = 0 \n",
    "for line in hamlet_1_text:\n",
    "    print(line, end='')\n",
    "    characters += sum(len(word) for word in line)\n",
    "    if characters == 325:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should see the following output:\n",
    "\n",
    "```\n",
    "The Tragedie of Hamlet\n",
    "\n",
    "Actus Primus. Scoena Prima.\n",
    "\n",
    "Enter Barnardo and Francisco two Centinels.\n",
    "\n",
    " Barnardo. Who's there?\n",
    "Fran. Nay answer me: Stand & vnfold\n",
    "your selfe\n",
    "\n",
    " Bar. Long liue the King\n",
    "\n",
    " Fran. Barnardo?\n",
    "Bar. He\n",
    "\n",
    " Fran. You come most carefully vpon your houre\n",
    "\n",
    " Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Instantiate an object <font face='courier'>tokenizer</font> of type <font face='courier'>tf.keras.preprocessing.text.Tokenzier</font>.<br>\n",
    "    <br>\n",
    "    Submit the argument <font face='courier'>char_level=True</font> to the constructor.<br>\n",
    "    <br>\n",
    "    Helpful information can be found <a href='https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer'>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The object <font face='courier'>tokenizer</font> will ultimately be used to encode the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font> as integer sequences in which each single number represents a specific character.<br>\n",
    "    <br>\n",
    "    Call the method <font face='courier'>fit_on_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    In doing so, pass a list that contains exactly the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer.fit_on_texts([hamlet_1_text, hamlet_2_text, hamlet_3_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The above call creates a vocabulary in <font face='courier'>tokenizer</font>. This vocabulary assigns a positive integer to each unique character in the texts that were passed to the <font face='courier'>fit_on_texts</font> method.<br>\n",
    "    <br>\n",
    "    Display the attribute <font face='courier'>word_index</font> of <font face='courier'>tokenizer</font> which contains the created vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'o': 4,\n",
       " 'a': 5,\n",
       " 'h': 6,\n",
       " 'i': 7,\n",
       " 'n': 8,\n",
       " 's': 9,\n",
       " 'r': 10,\n",
       " 'l': 11,\n",
       " '\\n': 12,\n",
       " 'u': 13,\n",
       " 'd': 14,\n",
       " 'm': 15,\n",
       " 'y': 16,\n",
       " ',': 17,\n",
       " 'w': 18,\n",
       " 'f': 19,\n",
       " 'c': 20,\n",
       " 'g': 21,\n",
       " '.': 22,\n",
       " 'p': 23,\n",
       " 'b': 24,\n",
       " 'k': 25,\n",
       " \"'\": 26,\n",
       " ':': 27,\n",
       " 'v': 28,\n",
       " '?': 29,\n",
       " ';': 30,\n",
       " 'q': 31,\n",
       " 'x': 32,\n",
       " '-': 33,\n",
       " 'z': 34,\n",
       " '(': 35,\n",
       " ')': 36,\n",
       " '&': 37,\n",
       " '!': 38,\n",
       " '[': 39,\n",
       " ']': 40,\n",
       " '1': 41,\n",
       " 'j': 42}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have got the following output:\n",
    "\n",
    "```\n",
    "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'n': 8, 's': 9, 'r': 10, 'l': 11, '\\n': 12, 'u': 13, 'd': 14, 'm': 15, 'y': 16, ',': 17, 'w': 18, 'f': 19, 'c': 20, 'g': 21, '.': 22, 'p': 23, 'b': 24, 'k': 25, \"'\": 26, ':': 27, 'v': 28, '?': 29, ';': 30, 'q': 31, 'x': 32, '-': 33, 'z': 34, '(': 35, ')': 36, '&': 37, '!': 38, '[': 39, ']': 40, '1': 41, 'j': 42}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compute the length of the vocabulary, store it in the variable <font face='courier'>max_id</font> and display this variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "max_id = len(tokenizer.word_index)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Now, use the method <font face='courier'>texts_to_sequences</font> of <font face='courier'>tokenzier</font> to encode <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.<br>\n",
    "    <br>\n",
    "    Store the respective coded strings in <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.<br>\n",
    "    <br>\n",
    "    Convert all three lists to the format <font face='courier'>numpy.array</font> and subtract <font face='courier'>1</font> from all entries so that all values range between <font face='courier'>0</font> and <font face='courier'>max_id - 1</font> afterwards.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded = np.array(tokenizer.texts_to_sequences(hamlet_1_text)) - 1\n",
    "hamlet_2_encoded = np.array(tokenizer.texts_to_sequences(hamlet_2_text)) - 1\n",
    "hamlet_3_encoded = np.array(tokenizer.texts_to_sequences(hamlet_3_text)) - 1\n",
    "\n",
    "\n",
    "#YOUR CODE\n",
    "# hamlet_1_encoded = np.array(tokenizer.texts_to_sequences(hamlet_1_text)) - 1\n",
    "# hamlet_2_encoded = np.array(tokenizer.texts_to_sequences(hamlet_2_text)) - 1\n",
    "# hamlet_3_encoded = np.array(tokenizer.texts_to_sequences(hamlet_3_text)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first <font face='courier'>325</font> entries of <font face='courier'>hamlet_1_encoded</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamlet_1_encoded = hamlet_1_encoded.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [20]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [18]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [14]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [11]\n",
      " [11]\n",
      " [ 4]\n",
      " [19]\n",
      " [ 2]\n",
      " [12]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [22]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [14]\n",
      " [12]\n",
      " [ 8]\n",
      " [21]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [19]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [22]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [14]\n",
      " [ 4]\n",
      " [21]\n",
      " [11]\n",
      " [11]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [13]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [13]\n",
      " [ 0]\n",
      " [18]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [19]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [19]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [17]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [19]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 8]\n",
      " [21]\n",
      " [11]\n",
      " [11]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [13]\n",
      " [ 3]\n",
      " [21]\n",
      " [ 0]\n",
      " [17]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [25]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [28]\n",
      " [11]\n",
      " [18]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [21]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [15]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [17]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [14]\n",
      " [ 1]\n",
      " [26]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [13]\n",
      " [ 0]\n",
      " [36]\n",
      " [ 0]\n",
      " [27]\n",
      " [ 7]\n",
      " [18]\n",
      " [ 3]\n",
      " [10]\n",
      " [13]\n",
      " [11]\n",
      " [15]\n",
      " [ 3]\n",
      " [12]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [18]\n",
      " [ 1]\n",
      " [11]\n",
      " [11]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [21]\n",
      " [ 0]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [20]\n",
      " [ 0]\n",
      " [10]\n",
      " [ 6]\n",
      " [12]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [24]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [20]\n",
      " [11]\n",
      " [11]\n",
      " [ 0]\n",
      " [18]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [21]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [13]\n",
      " [ 3]\n",
      " [28]\n",
      " [11]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [21]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [11]\n",
      " [11]\n",
      " [ 0]\n",
      " [18]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [21]\n",
      " [ 0]\n",
      " [15]\n",
      " [ 3]\n",
      " [12]\n",
      " [ 0]\n",
      " [19]\n",
      " [ 3]\n",
      " [14]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [14]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [19]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [18]\n",
      " [12]\n",
      " [10]\n",
      " [10]\n",
      " [15]\n",
      " [ 0]\n",
      " [27]\n",
      " [22]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [15]\n",
      " [ 3]\n",
      " [12]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [12]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [11]\n",
      " [11]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [21]\n",
      " [ 0]\n",
      " [25]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [17]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [24]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [17]\n",
      " [ 1]\n",
      " [10]\n",
      " [12]\n",
      " [ 1]\n",
      " [16]\n",
      " [ 0]\n",
      " [20]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 0]\n",
      " [18]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [19]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [19]\n",
      " [ 3]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "print(hamlet_1_encoded[:325])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should be as follows:\n",
    "\n",
    "```\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4  9  7  4  9 13  3 21  0 17  5  3 25  8  0  2  5  1  9  1\n",
    " 28 11 18  9  4  7 21  0  7  4 15  0  4  7  8 17  1  9  0 14  1 26  0  8\n",
    "  2  4  7 13  0 36  0 27  7 18  3 10 13 11 15  3 12  9  0  8  1 10 18  1\n",
    " 11 11  0 23  4  9 21  0 10  3  7 20  0 10  6 12  1  0  2  5  1  0 24  6\n",
    "  7 20 11 11  0 18  9  4  7 21  0 23  4  9  7  4  9 13  3 28 11 23  4  9\n",
    " 21  0  5  1 11 11  0 18  9  4  7 21  0 15  3 12  0 19  3 14  1  0 14  3\n",
    "  8  2  0 19  4  9  1 18 12 10 10 15  0 27 22  3  7  0 15  3 12  9  0  5\n",
    "  3 12  9  1 11 11  0 23  4  9 21  0 25  2  6  8  0  7  3 17  0  8  2  9\n",
    "  3  3 24  0  2 17  1 10 12  1 16  0 20  1  2  0  2  5  1  1  0  2  3  0\n",
    " 23  1 13  0 18  9  4  7 19  6  8 19  3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The original texts can be recovered via the method <font face='courier'>sequences_to_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    Texts recovered in this way are all lower case and each original character is followed by a blank space.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>sequences_to_texts</font> to <font face='courier'>hamlet_1_encoded + 1</font> and store the result in <font face='courier'>hamlet_1_decoded</font>.<br>\n",
    "    <br>\n",
    "    Afterfwards, display the first <font face='courier'>649</font> characters of <font face='courier'>hamlet_1_decoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e   t r a g e d i e   o f   h a m l e t \n",
      " \n",
      " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
      " \n",
      " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
      " \n",
      "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
      " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
      " y o u r   s e l f e \n",
      " \n",
      "   b a r .   l o n g   l i u e   t h e   k i n g \n",
      " \n",
      "   f r a n .   b a r n a r d o ? \n",
      " b a r .   h e \n",
      " \n",
      "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
      " \n",
      "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o "
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_decoded = tokenizer.sequences_to_texts(hamlet_1_encoded + 1)\n",
    "# hamlet_1_decoded = tokenizer.sequences_to_texts([x+1 for x in hamlet_1_encoded])\n",
    "\n",
    "characters = 0 \n",
    "for ch in hamlet_1_decoded:\n",
    "    print(ch, end=' ')\n",
    "    characters += 1\n",
    "    if characters == 325:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look like this:\n",
    "\n",
    "```\n",
    "t h e   t r a g e d i e   o f   h a m l e t \n",
    " \n",
    " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
    " \n",
    " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
    " \n",
    "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
    " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
    " y o u r   s e l f e \n",
    " \n",
    "   b a r .   l o n g   l i u e   t h e   k i n g \n",
    " \n",
    "   f r a n .   b a r n a r d o ? \n",
    " b a r .   h e \n",
    " \n",
    "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
    " \n",
    "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Create three objects <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> of type <font face='courier'>tf.data.dataset</font> by applying the method<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.from_tensor_slices</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices'>this link</a>) to <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = tf.data.Dataset.from_tensor_slices(hamlet_1_encoded)\n",
    "hamlet_2_dataset = tf.data.Dataset.from_tensor_slices(hamlet_2_encoded)\n",
    "hamlet_3_dataset = tf.data.Dataset.from_tensor_slices(hamlet_3_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first ten elements of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([5], shape=(1,), dtype=int32)\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([9], shape=(1,), dtype=int32)\n",
      "tf.Tensor([4], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "tf.Tensor([13], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "count = 0\n",
    "for ele in hamlet_1_dataset:\n",
    "    print(ele)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have produced the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, each item of <font face='courier'>hamlet_1_dataset</font> is an integer tensor including one single value.<br>\n",
    "    <br>\n",
    "    Accordingly, <font face='courier'>hamlet_1_dataset</font> has <font face='courier'>len(hamlet_1_encoded)</font> elements in total (the same applies in case of the other two datasets).<br>\n",
    "    <br>\n",
    "    In the following, you will train a recurrent neural network which gets a coded string of length <font face='courier'>T = 100</font> and predicts subsequent characters in all time steps.<br>\n",
    "    <br>\n",
    "    Accordingly, we will first transform our three datasets such that their elements become one-dimensional tensors of length <font face='courier'>window_length = T + 1</font>.<br>\n",
    "    <br>\n",
    "    Initialize <font face='courier'>T</font> and <font face='courier'>window_length</font> as described.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "T = 100\n",
    "window_length = T + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use the method <font face='courier'>tf.data.Dataset.window</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window'>this link</a>) to get items that feature the desired length.<br>\n",
    "    <br>\n",
    "    Call the method using the arguments <font face='courier'>shift = 1</font> and <font face='courier'>drop_remainder = True</font>.<br>\n",
    "    <br>\n",
    "    The first-mentioned argument <font face='courier'>shift = 1</font> ensures that the first item of the transformed dataset contains the elements <font face='courier'>0,...,window_length - 1</font> of the original dataset, the next item <font face='courier'>1,...,window_length</font>, and so on.<br>\n",
    "    <br>\n",
    "    In other words: A window of length <font face='courier'>window_length</font> slides with feed <font face='courier'>shift</font> over the original dataset and extracts sequence by sequence until the end of the dataset is reached.<br>\n",
    "    <br>\n",
    "    The latter argument <font face='courier'>drop_remainder = True</font> ensures that no shorter sequences are extracted towards the end of the dataset, where the window could potentially slide beyond the end of the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = hamlet_1_dataset.window(size = window_length, shift = 1, drop_remainder = True)\n",
    "hamlet_2_dataset = hamlet_2_dataset.window(size = window_length, shift = 1, drop_remainder = True)\n",
    "hamlet_3_dataset = hamlet_3_dataset.window(size = window_length, shift = 1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset element_spec=TensorSpec(shape=(1,), dtype=tf.int32, name=None)>\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([5], shape=(1,), dtype=int32)\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([9], shape=(1,), dtype=int32)\n",
      "tf.Tensor([4], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "tf.Tensor([13], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for window in hamlet_1_dataset.take(1):\n",
    "    print(window)\n",
    "    for item in window.take(10):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have obtained an output like this:\n",
    "\n",
    "```\n",
    "<_VariantDataset shapes: (), types: tf.int32>\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, the transformed datasets <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> have now elements which are again objects of type <font face='courier'>tf.data.Dataset</font> (or of a derived class).<br>\n",
    "-    <br>\n",
    "    Each of these sub-datasets <font face='courier'>window</font> contains a number of <font face='courier'>window_size</font> single-valued tensors.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.flat_map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map'>this link</a>) to all three datasets to transform the sub-datasets <font face='courier'>window</font> into one-dimensional tensors of length <font face='courier'>window_length</font>.<br>\n",
    "    <br>\n",
    "    Pass a function which maps <font face='courier'>window</font> to <font face='courier'>window.batch(window_length)</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = hamlet_1_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "hamlet_2_dataset = hamlet_2_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "hamlet_3_dataset = hamlet_3_dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first element of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [20]\n",
      " [ 1]\n",
      " [13]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [18]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [14]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [11]\n",
      " [11]\n",
      " [ 4]\n",
      " [19]\n",
      " [ 2]\n",
      " [12]\n",
      " [ 8]\n",
      " [ 0]\n",
      " [22]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [14]\n",
      " [12]\n",
      " [ 8]\n",
      " [21]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [19]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 0]\n",
      " [22]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [14]\n",
      " [ 4]\n",
      " [21]\n",
      " [11]\n",
      " [11]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [13]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [13]\n",
      " [ 0]\n",
      " [18]\n",
      " [ 9]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [19]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [19]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [17]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [19]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 8]\n",
      " [21]\n",
      " [11]\n",
      " [11]\n",
      " [ 0]\n",
      " [23]\n",
      " [ 4]], shape=(101, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "for window in hamlet_1_dataset.take(1):\n",
    "    print(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should get the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4], shape=(101,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.concatenate</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#concatenate'>this link</a>) to merge<br>\n",
    "    <br>\n",
    "    <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> to a single dataset <font face='courier'>hamlet_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset = hamlet_1_dataset.concatenate(hamlet_2_dataset)\n",
    "hamlet_dataset = hamlet_dataset.concatenate(hamlet_3_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Set <font face='courier'>batch_size = 32</font>.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>tf.data.Dataset.repeat</font> (without argument),<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.shuffle</font> (with <font face='courier'>buffer_size = 1000</font>)<br>\n",
    "    <br>\n",
    "    and finally <font face='courier'>tf.data.Dataset.batch</font> (with <font face='courier'>drop_remainder=True</font>).\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# YOUR CODE\n",
    "batch_size = 32\n",
    "hamlet_dataset.repeat()\n",
    "hamlet_dataset.shuffle(buffer_size = 1000)\n",
    "window_batch =  hamlet_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Our dataset contains now two-dimensional tensors <font face='courier'>window_batch</font> of size <font face='courier'>(32, 101)</font>.<br>\n",
    "    <br>\n",
    "    Each slice <font face='courier'>window_batch[i, :]</font> corresponds to a training example.<br>\n",
    "    <br>\n",
    "    Here, we still need to subdivide the training examples into inputs and outputs.<br>\n",
    "    <br>\n",
    "    Each single encoded character <font face='courier'>window_batch[i, j]</font> (for <font face='courier'>j=0,...,99</font>) is an  input $\\mathbf{x}^{<t>(i)}$ in a time step<br>\n",
    "    <br>\n",
    "    and the associated output is <font face='courier'>window_batch[i, j + 1]</font> which corresponds to $\\mathbf{y}^{<t>(i)}$.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map'>this link</a>) to <font face='courier'>hamlet_dataset</font><br>.\n",
    "    <br>\n",
    "    Each batch <font face='courier'>window_batch</font> shall be mapped to a tuple of two tensors of size <font face='courier'>(32, 100)</font>.<br>\n",
    "    <br>\n",
    "    The <font face='courier'>[i, :]</font>-th slice of the first tensor shall contain the entries <font face='courier'>window_batch[i, 0:100]</font>.<br>\n",
    "    <br>\n",
    "    The corresponding slice of the second tensor shall contain <font face='courier'>window_batch[i, 1:101]</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: tf__window_batch_mapping() takes 0 positional arguments but 1 was given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19636/1481241579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwindow_batch_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhamlet_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_batch_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2292\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m   2293\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m-> 2294\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2296\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5497\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5498\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5499\u001b[1;33m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   5500\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5501\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m     concrete_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m    227\u001b[0m         *args, **kwargs)\n\u001b[0;32m    228\u001b[0m     \u001b[0mconcrete_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m       \u001b[0mconcrete_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_concrete_function_internal_garbage_collected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneralized_func_key\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m           \u001b[0mconcrete_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m           \u001b[0mgraph_capture_container\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capture_func_lib\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     concrete_function = monomorphic_function.ConcreteFunction(\n\u001b[1;32m--> 284\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    238\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m    239\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    169\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__window_batch_mapping() takes 0 positional arguments but 1 was given\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "# def window_batch_mapping():\n",
    "#     for i in range(32):\n",
    "#         window_batch_new = (window_batch[i, 0:100], window_batch[i, 1:101])\n",
    "#     return window_batch_new\n",
    "    \n",
    "hamlet_dataset.map(lambda window_batch_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19908/282565813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwindow_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhamlet_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7214\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7215\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "for window_batch in hamlet_dataset.take(1):\n",
    "    [x] = tokenizer.sequences_to_texts([window_batch[0][0, :].numpy() + 1])\n",
    "    [y] = tokenizer.sequences_to_texts([window_batch[1][0, :].numpy() + 1])\n",
    "    print(x)\n",
    "    print()\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look as follows:\n",
    "\n",
    "```\n",
    "  o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p\n",
    "\n",
    "o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply <font face='courier'>tf.data.Dataset.map</font> again to <font face='courier'>hamlet_dataset</font> to map each element <font face='courier'>(X_batch, Y_batch)</font> to a new tuple.<br>\n",
    "    <br>\n",
    "    In the new tuple, <font face='courier'>Y_batch</font> shall remain unchanged while <font face='courier'>X_batch</font> undergoes another encoding via <font face='courier'>tf.one_hot</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/one_hot'>this link</a>).<br>\n",
    "    <br>\n",
    "    Find out, which value <font face='courier'>depth</font> you need to pass <font face='courier'>tf.one_hot</font> in addition to <font face='courier'>X_batch</font>.<br>\n",
    "    <br>\n",
    "    Hint: You already computed the correct value above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.prefetch</font> (with <font face='courier'>buffer_size = 1</font>) to <font face='courier'>hamlet_dataset</font> to get your dataset ready for training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Deduce a formula for the number of elements in <font face='courier'>hamlet_dataset</font>.<br>\n",
    "    <br>\n",
    "    Display the result and store it in <font face='courier'>steps_per_epoch</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The number of items in the dataset should be as follows:\n",
    "\n",
    "```\n",
    "5014\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use <font face='courier'>keras.models.Sequential</font> to define a <font face='courier'>model</font> featuring<br>\n",
    "    <br>\n",
    "    two hidden GRU layers (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU'>this link</a>) with <font face='courier'>128</font> neurons each, as well as<br>\n",
    "    <br>\n",
    "    a fully connected output layer (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'>this link</a>) with <font face='courier'>max_id</font> neurons and <font face='courier'>softmax</font> activation function.<br>\n",
    "    <br>\n",
    "    This model corresponds to the RNN displayed in Fig. 1 with an additional hidden GRU layer.<br>\n",
    "    <br>\n",
    "    To make the model generate outputs in each time step, the output layer must be enclosed by a <font face='courier'>keras.layers.TimeDistributed</font> wrapper (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed'>this link</a>).<br>\n",
    "    <br>\n",
    "    Without the layer, the model would generate only a single output in the very last time step (as the one displaye in Fig. 2).<br>\n",
    "    <br>\n",
    "    In case of the GRU layers, you should add <font face='courier'>return_sequences = True</font> for the same purpose.<br>\n",
    "    <br>\n",
    "    Also recall that the input layer needs an argument <font face='courier'>input_shape=[None, max_id]</font> (where <font face='courier'>None</font> represents the temporal dimension of the input sequence).<br>\n",
    "    <br>\n",
    "    During training, we could just as well replace <font face='courier'>None</font> with <font face='courier'>T</font> as all sequences in the training data have identical length.<br>\n",
    "    <br>\n",
    "    However, passing <font face='courier'>None</font> ensures that the model will accept arbitrarily long sequences later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compile <font face='courier'>model</font> using <font face='courier'>sparse_categorical_crossentropy</font> and <font face='courier'>adam</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Train <font face='courier'>model</font> for one epoch. Don't forget to pass the <font face='courier'>steps_per_epoch</font> argument.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    You might have observed that even a single epoch takes quite some time.<br>\n",
    "    <br>\n",
    "    Create a file <font face='courier'>hamlet_rnn.py</font> and train the model for <font face='courier'>20</font> epochs on the GPU cluster.<br>\n",
    "    <br>\n",
    "    In doing so, use a callback<br>\n",
    "    <br>\n",
    "    <font face='courier'>keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)</font><br>\n",
    "    <br>\n",
    "    (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping'>this link</a>). Save your model as  <font face='courier'>hamlet_model.h5</font> and load it in the following step for further use in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Write a function <font face='courier'>preprocess</font> that takes a argument a list <font face='courier'>texts</font> containing a single string.<br>\n",
    "    <br>\n",
    "    Inside this function, use <font face='courier'>tokenizer</font> again to encode the string in <font face='courier'>texts</font> as a NumPy-Array <font face='courier'>X</font>.<br>\n",
    "    <br>\n",
    "    The return value of <font face='courier'>preprocess</font> shall be the one-hot encoded version of <font face='courier'>X</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>next_char</font> takes as argument a string <font face='courier'>text</font> and a positive number <font face='courier'>temperature</font><br>\n",
    "    <br>\n",
    "    with the goal to generate the next character after <font face='courier'>text</font>.<br>\n",
    "    <br>\n",
    "    Replace all placeholders <font face='courier'>None</font> as follows:<br>\n",
    "    <br>\n",
    "    First, apply <font face='courier'>preprocess</font> to encode <font face='courier'>text</font> and store the result in <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Second, apply <font face='courier'>model.predict</font> the generate outputs given the input sequence <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Store the output from the last time step in <font face='courier'>y_proba</font>.<br>\n",
    "    <br>\n",
    "    <font face='courier'>rescaled_logits</font> and <font face='courier'>char_id</font> are finally used to generate a one-element sample from  $\\{1,\\dots,\\mathrm{max\\_id}\\}$.<br>\n",
    "    <br>\n",
    "    The generated sample is the encoded version of the character to be generated.<br>\n",
    "    <br>\n",
    "    Hence, <font face='courier'>char_id.numpy()</font> will return the character itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = None\n",
    "    y_proba = None\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>complete_text</font> takes a string <font face='courier'>text</font> and shall append <font face='courier'>n_char</font> subsequent characters generated by your RNN.<br>\n",
    "    <br>\n",
    "    Complete the function accordingly. The argument <font face='courier'>temperature</font> just needs to be passed to <font face='courier'>next_char</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Starting from the initial string <font face='courier'>'Hamlet'</font>, use <font face='courier'>complete_text</font> to generate a text with <font face='courier'>1000</font> characters.<br>\n",
    "    <br>\n",
    "    You can vary the argument <font face='courier'>temperature</font> to modify the distribution from which new characters are drawn.<br>\n",
    "    <br>\n",
    "    Values close to zero encourage characters that have a high probability according to the distribution generated by your RNN.<br>\n",
    "    <br>\n",
    "    If <font face='courier'>temperature</font> is too high, new characters are drawn according to a uniform distribution on the entire vocabulary, which is not desirable.<br>\n",
    "    <br>\n",
    "    You can, for example, try different values between <font face='courier'>0</font> and <font face='courier'>2</font> and evaluate generated texts based on how plausible they appear to you.<br>\n",
    "    <br>\n",
    "    Display your generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Submit your completed notebook not later than on January 15th, 2023.<br>\n",
    "    <br>\n",
    "    Optional: Train an RNN on a text corpus of your own choice and use your own RNN to generate text.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
